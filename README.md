# ğŸ•·ï¸ IMS Crawler

A Python-based web crawler for extracting structured data from custom IMS (Issue Management Systems) for knowledge base integration and troubleshooting support.

## ğŸ“‹ Overview

This crawler enables systematic extraction of issues, comments, attachments, and history from IMS systems, preparing data for integration with RAG (Retrieval-Augmented Generation) systems and LLM-powered troubleshooting guides.

### Key Features

- âœ… **Web Scraping**: Automated browser-based crawling using Playwright
- ğŸ” **Authentication**: Session management with automatic re-login on timeout
- ğŸ” **Advanced Search**: Support for IMS-specific search syntax (OR, AND, exact phrase)
- ğŸ“¦ **Attachment Processing**: Downloads and extracts text from PDFs, Word docs, images
- ğŸ“Š **Structured Output**: JSON format with complete issue metadata
- ğŸ¯ **User-Driven**: Crawl on-demand based on product and keyword filters
- ğŸš€ **CLI Interface**: Rich terminal UI with progress tracking

## ğŸ—ï¸ Architecture

```
User Input (Product + Keywords)
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Authentication Manager         â”‚
â”‚   (Login + Session Management)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Search Query Builder           â”‚
â”‚   (IMS Syntax: OR/AND/Exact)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Main Scraper Engine            â”‚
â”‚   (Playwright Browser Automation)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      IMS Page Parser                â”‚
â”‚   (Extract: Title, Desc, Comments,  â”‚
â”‚    History, Attachments, Metadata)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Attachment Processor           â”‚
â”‚   (Download + Text Extraction)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      JSON Exporter                  â”‚
â”‚   (Structured Issue Data)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Quick Start

### Prerequisites

- Python 3.10 or higher
- Git

### Installation

1. **Clone the repository**
```bash
cd web-crawler
```

2. **Create virtual environment**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Install Playwright browsers**
```bash
playwright install chromium
```

5. **Configure environment**
```bash
cp .env.example .env
# Edit .env with your IMS credentials and URL
```

### Configuration

Edit `.env` file:

```env
# IMS Configuration
IMS_BASE_URL=https://your-ims-system.com
IMS_USERNAME=your_username
IMS_PASSWORD=your_password

# Crawler Settings
MAX_CONCURRENT_REQUESTS=5
DEFAULT_MAX_RESULTS=100
DOWNLOAD_ATTACHMENTS=true

# Output Settings
OUTPUT_DIR=data/issues
ATTACHMENTS_DIR=data/attachments
```

### Basic Usage

**Check configuration:**
```bash
python main.py config
```

**Crawl issues:**
```bash
python main.py crawl --product "Tibero" --keywords "connection +error" --max-results 50
```

**Test search query:**
```bash
python main.py test-query "timeout crash +error"
```

## ğŸ“– IMS Search Syntax

The crawler supports the following IMS-specific search operators:

### 1. OR Search (Space Delimiter)
```bash
python main.py crawl -p "Tibero" -k "Tmax Tibero"
# Finds: Tmax OR Tibero
```

### 2. AND Search (+ Operator)
```bash
python main.py crawl -p "Tibero" -k "IMS +error"
# Finds: IMS AND error
```

### 3. Exact Phrase Search (Quotation Marks)
```bash
python main.py crawl -p "Tibero" -k '"connection timeout"'
# Finds: Exact phrase "connection timeout"
```

### 4. Complex Queries (Combination)
```bash
python main.py crawl -p "Tibero" -k 'timeout crash +error +"system failure"'
# Finds: (timeout OR crash) AND error AND "system failure"
```

### 5. Issue Number Search
```bash
python main.py crawl -p "Tibero" -k "+IMS-12345"
# Finds: Specific issue number
```

## ğŸ“ Output Structure

### JSON Output Format

Each crawled issue is saved as a separate JSON file:

```json
{
  "issue_id": "IMS-12345",
  "title": "Connection timeout error in Tibero",
  "description": "Detailed description of the issue...",
  "product": "Tibero",
  "status": "Resolved",
  "priority": "High",
  "created_date": "2024-01-15",
  "updated_date": "2024-01-20",
  "reporter": "john.doe",
  "assignee": "jane.smith",
  "comments": [
    {
      "author": "john.doe",
      "date": "2024-01-15",
      "content": "Initial comment..."
    }
  ],
  "history": [
    {
      "user": "jane.smith",
      "date": "2024-01-16",
      "action": "Status changed",
      "details": "From Open to In Progress"
    }
  ],
  "attachments": [
    {
      "name": "error.log",
      "url": "/attachments/12345/error.log",
      "size": "15 KB",
      "local_path": "data/attachments/IMS-12345/error.log",
      "extracted_text": "Preview of extracted text..."
    }
  ],
  "metadata": {},
  "url": "https://ims.example.com/issue/IMS-12345",
  "crawled_at": "2024-01-21T10:30:00"
}
```

### Directory Structure

```
data/
â”œâ”€â”€ issues/                 # Crawled issue JSON files
â”‚   â”œâ”€â”€ IMS-12345_20240121_103000.json
â”‚   â”œâ”€â”€ IMS-12346_20240121_103015.json
â”‚   â””â”€â”€ ...
â””â”€â”€ attachments/           # Downloaded attachments
    â”œâ”€â”€ IMS-12345/
    â”‚   â”œâ”€â”€ error.log
    â”‚   â”œâ”€â”€ error.log.txt  # Extracted text
    â”‚   â””â”€â”€ screenshot.png
    â””â”€â”€ IMS-12346/
        â””â”€â”€ ...
```

## ğŸ”§ Advanced Usage

### Custom Output Directory

```bash
python main.py crawl \
  --product "JEUS" \
  --keywords "memory +leak" \
  --max-results 100 \
  --output-dir "./custom_output"
```

### Run with Visible Browser (Debug Mode)

```bash
python main.py crawl \
  --product "Tibero" \
  --keywords "error" \
  --max-results 10 \
  --no-headless
```

### Batch Processing

Create a shell script for multiple queries:

```bash
#!/bin/bash
# batch_crawl.sh

products=("Tibero" "JEUS" "WebtoB")
keywords=("error" "timeout" "crash")

for product in "${products[@]}"; do
  for keyword in "${keywords[@]}"; do
    echo "Crawling: $product - $keyword"
    python main.py crawl -p "$product" -k "+$keyword" -m 50
  done
done
```

## ğŸ› ï¸ Customization Guide

### Adapting to Your IMS System

The crawler includes **TODO** markers in the code where customization is required based on your specific IMS system:

#### 1. Authentication (`crawler/auth.py`)

Update login selectors:
```python
# Update these selectors based on your IMS login page
page.fill('input[name="username"]', self.username)  # Update selector
page.fill('input[name="password"]', self.password)  # Update selector
page.click('button[type="submit"]')  # Update selector
```

#### 2. Search Execution (`crawler/ims_scraper.py`)

Update search page selectors:
```python
# Update search URL
search_url = f"{self.base_url}/search"  # Update URL

# Update search input selector
self.page.fill('input[name="search"]', query)  # Update selector

# Update result list selector
result_elements = self.page.query_selector_all('.issue-link')  # Update selector
```

#### 3. Issue Parsing (`crawler/parser.py`)

Update all extraction selectors based on your IMS HTML structure:
```python
# Example: Update title extraction
def _extract_title(self, page: Page) -> str:
    element = page.query_selector('.issue-title')  # Update this selector
    return element.text_content().strip() if element else ''
```

### Testing Your Customizations

1. **Run with visible browser:**
```bash
python main.py crawl -p "Test" -k "test" -m 1 --no-headless
```

2. **Use browser DevTools to inspect elements and find correct selectors**

3. **Test incrementally** - verify login, search, then full crawl

## ğŸ“Š Data Processing Pipeline

### Recommended Workflow for RAG Integration

```
1. Crawl IMS Issues
   â””â”€> python main.py crawl -p "Product" -k "keywords" -m 1000

2. Process JSON Files
   â””â”€> Load all JSON from data/issues/
   â””â”€> Combine into single dataset

3. Text Preprocessing
   â””â”€> Concatenate: title + description + comments
   â””â”€> Include attachment text extracts
   â””â”€> Clean and normalize text

4. Embedding Generation
   â””â”€> Use sentence-transformers or OpenAI embeddings
   â””â”€> Store in Vector DB (Chroma, Qdrant, Pinecone)

5. RAG System Integration
   â””â”€> Query: User troubleshooting question
   â””â”€> Retrieve: Top-K similar issues from Vector DB
   â””â”€> Generate: LLM response based on retrieved context
```

## ğŸ› Troubleshooting

### Common Issues

**Authentication fails:**
- Verify credentials in `.env`
- Check if IMS has CAPTCHA or 2FA (not currently supported)
- Run with `--no-headless` to see what's happening

**Search returns no results:**
- Test query syntax with `python main.py test-query "your query"`
- Verify product name is correct
- Check IMS manually to confirm issues exist

**Playwright timeout errors:**
- Increase timeout in `config/settings.py`: `BROWSER_TIMEOUT = 60000`
- Check network connectivity
- IMS might be slow - add delays

**Missing attachments:**
- Set `DOWNLOAD_ATTACHMENTS=true` in `.env`
- Check disk space
- Verify download permissions

## ğŸ“ Project Structure

```
web-crawler/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ settings.py           # Configuration management
â”œâ”€â”€ crawler/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ auth.py               # Authentication & session management
â”‚   â”œâ”€â”€ search.py             # IMS search query builder
â”‚   â”œâ”€â”€ ims_scraper.py        # Main scraper engine
â”‚   â”œâ”€â”€ parser.py             # HTML parsing & data extraction
â”‚   â””â”€â”€ attachment_processor.py  # Attachment download & text extraction
â”œâ”€â”€ data/                     # Output directory (gitignored)
â”‚   â”œâ”€â”€ issues/               # Crawled JSON files
â”‚   â””â”€â”€ attachments/          # Downloaded files
â”œâ”€â”€ tests/                    # Unit tests
â”œâ”€â”€ .env.example              # Environment template
â”œâ”€â”€ .gitignore
â”œâ”€â”€ main.py                   # CLI interface
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ CLAUDE.md                 # Project guidance for Claude
â”œâ”€â”€ plan.md                   # Project plan (Korean)
â””â”€â”€ README.md                 # This file
```

## ğŸ”® Future Enhancements

### Phase 2: Web Application
- [ ] Flask/FastAPI web interface
- [ ] Real-time crawl progress dashboard
- [ ] Scheduled crawling (cron-like)
- [ ] Multi-user support
- [ ] RAG query interface

### Additional Features
- [ ] Incremental crawling (only new/updated issues)
- [ ] Database storage (PostgreSQL)
- [ ] Advanced filtering (date range, status, priority)
- [ ] Export formats (CSV, Excel)
- [ ] Parallel crawling for faster processing
- [ ] Retry logic for failed downloads
- [ ] Email notifications on completion

## ğŸ“„ License

[Specify your license here]

## ğŸ¤ Contributing

[Contribution guidelines if open source]

## ğŸ“§ Contact

[Your contact information]

---

**Built with â¤ï¸ for IMS knowledge extraction and RAG integration**
