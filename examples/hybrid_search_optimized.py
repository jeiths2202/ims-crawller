"""
Optimized Hybrid Search for Korean/Japanese/English
Uses character n-grams for better CJK language support
"""
import sys
import io

# Fix Windows console encoding
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8')

import json
import re
from pathlib import Path
from rank_bm25 import BM25Okapi
from sentence_transformers import SentenceTransformer, util
import torch
import numpy as np
from typing import List, Dict, Tuple

class CJKTokenizer:
    """í•œêµ­ì–´/ì¼ë³¸ì–´/ì¤‘êµ­ì–´ ìµœì í™” í† í¬ë‚˜ì´ì €"""

    @staticmethod
    def tokenize(text: str, use_ngrams: bool = True) -> List[str]:
        """
        CJK ì–¸ì–´ ìµœì í™” í† í°í™”

        Args:
            text: ì…ë ¥ í…ìŠ¤íŠ¸
            use_ngrams: Trueë©´ ë¬¸ì bi-gram ì‚¬ìš©, Falseë©´ ê³µë°±ë§Œ

        Returns:
            í† í° ë¦¬ìŠ¤íŠ¸
        """
        # ì†Œë¬¸ì ë³€í™˜
        text = text.lower()

        # ê³µë°± ê¸°ë°˜ í† í°
        tokens = text.split()

        if not use_ngrams:
            return tokens

        # ë¬¸ì bi-gram ì¶”ê°€ (í•œêµ­ì–´/ì¼ë³¸ì–´/ì¤‘êµ­ì–´ ëŒ€ì‘)
        ngrams = []
        for token in tokens:
            # ì˜ì–´/ìˆ«ìëŠ” ê·¸ëŒ€ë¡œ
            if re.match(r'^[a-z0-9]+$', token):
                ngrams.append(token)
            else:
                # CJK ë¬¸ìëŠ” bi-gram ìƒì„±
                for i in range(len(token) - 1):
                    ngrams.append(token[i:i+2])
                # ì›ë³¸ í† í°ë„ ì¶”ê°€
                ngrams.append(token)

        return tokens + ngrams

class OptimizedHybridSearch:
    """í•œêµ­ì–´/ì¼ë³¸ì–´ ìµœì í™” í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"""

    def __init__(self,
                 semantic_model='paraphrase-multilingual-MiniLM-L12-v2',
                 bm25_weight=0.3,
                 semantic_weight=0.7,
                 use_ngrams=True):
        """
        Args:
            semantic_model: ì‹œë§¨í‹± ì„ë² ë”© ëª¨ë¸
            bm25_weight: BM25 ì ìˆ˜ ê°€ì¤‘ì¹˜
            semantic_weight: ì‹œë§¨í‹± ì ìˆ˜ ê°€ì¤‘ì¹˜
            use_ngrams: Character n-gram ì‚¬ìš© ì—¬ë¶€
        """
        print(f"Loading semantic model: {semantic_model}")
        self.semantic_model = SentenceTransformer(semantic_model)
        self.bm25_weight = bm25_weight
        self.semantic_weight = semantic_weight
        self.use_ngrams = use_ngrams
        self.tokenizer = CJKTokenizer()

        self.documents = []
        self.contents = []
        self.bm25 = None
        self.embeddings = None

    def index_comments(self, comments: List[Dict]):
        """ëŒ“ê¸€ ì¸ë±ì‹±"""
        if not comments:
            return

        self.documents = comments
        self.contents = [c['content'] for c in comments]

        # BM25 ì¸ë±ì‹± (CJK í† í¬ë‚˜ì´ì € ì‚¬ìš©)
        tokenized_contents = [
            self.tokenizer.tokenize(content, self.use_ngrams)
            for content in self.contents
        ]
        self.bm25 = BM25Okapi(tokenized_contents)

        # ì‹œë§¨í‹± ì„ë² ë”©
        print(f"Encoding {len(self.contents)} documents...")
        self.embeddings = self.semantic_model.encode(
            self.contents,
            convert_to_tensor=True,
            show_progress_bar=True
        )

        print(f"âœ… Indexed {len(self.contents)} comments")

    def search(self, query: str, top_k: int = 5, threshold: float = 0.0) -> List[Tuple[Dict, float, Dict]]:
        """
        í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ì ìˆ˜ ë¶„í•´ í¬í•¨)

        Returns:
            List of (comment_dict, hybrid_score, score_breakdown)
        """
        if not self.documents:
            return []

        # BM25 ê²€ìƒ‰
        tokenized_query = self.tokenizer.tokenize(query, self.use_ngrams)
        bm25_scores = self.bm25.get_scores(tokenized_query)
        bm25_scores_norm = bm25_scores / (bm25_scores.max() + 1e-8)

        # ì‹œë§¨í‹± ê²€ìƒ‰
        query_embedding = self.semantic_model.encode(query, convert_to_tensor=True)
        semantic_scores = util.cos_sim(query_embedding, self.embeddings)[0].cpu().numpy()

        # í•˜ì´ë¸Œë¦¬ë“œ ì ìˆ˜
        hybrid_scores = (
            self.bm25_weight * bm25_scores_norm +
            self.semantic_weight * semantic_scores
        )

        # ìƒìœ„ kê°œ ì„ íƒ
        top_indices = np.argsort(hybrid_scores)[::-1][:top_k]

        results = []
        for idx in top_indices:
            score = float(hybrid_scores[idx])
            if score >= threshold:
                breakdown = {
                    'bm25': float(bm25_scores_norm[idx]),
                    'semantic': float(semantic_scores[idx]),
                    'hybrid': score
                }
                results.append((self.documents[idx], score, breakdown))

        return results


def compare_search_methods(comments: List[Dict], query: str):
    """ë‹¤ì–‘í•œ ê²€ìƒ‰ ë°©ë²• ë¹„êµ"""

    print(f"\n{'='*80}")
    print(f"ğŸ” Query: '{query}'")
    print(f"{'='*80}\n")

    # 1. Semantic only
    print("ã€1ã€‘ Semantic Search Only (100% Semantic)")
    print("-" * 80)
    semantic_only = OptimizedHybridSearch(
        bm25_weight=0.0,
        semantic_weight=1.0
    )
    semantic_only.index_comments(comments)
    results_semantic = semantic_only.search(query, top_k=3)

    for i, (comment, score, breakdown) in enumerate(results_semantic, 1):
        print(f"\n[{i}] Score: {score:.3f} (Semantic: {breakdown['semantic']:.3f})")
        print(f"    ì‘ì„±ì: {comment.get('author', 'Unknown')}")
        print(f"    ë‚´ìš©: {comment['content'][:150]}...")

    # 2. Hybrid (no n-grams)
    print(f"\n\n{'='*80}")
    print("ã€2ã€‘ Hybrid Search (30% BM25 + 70% Semantic, ê³µë°± í† í°ë§Œ)")
    print("-" * 80)
    hybrid_basic = OptimizedHybridSearch(
        bm25_weight=0.3,
        semantic_weight=0.7,
        use_ngrams=False  # ê³µë°±ë§Œ
    )
    hybrid_basic.index_comments(comments)
    results_hybrid_basic = hybrid_basic.search(query, top_k=3)

    for i, (comment, score, breakdown) in enumerate(results_hybrid_basic, 1):
        print(f"\n[{i}] Score: {score:.3f} (BM25: {breakdown['bm25']:.3f}, Semantic: {breakdown['semantic']:.3f})")
        print(f"    ì‘ì„±ì: {comment.get('author', 'Unknown')}")
        print(f"    ë‚´ìš©: {comment['content'][:150]}...")

    # 3. Hybrid (with n-grams)
    print(f"\n\n{'='*80}")
    print("ã€3ã€‘ Hybrid Search (30% BM25 + 70% Semantic, Character N-grams)")
    print("-" * 80)
    hybrid_ngrams = OptimizedHybridSearch(
        bm25_weight=0.3,
        semantic_weight=0.7,
        use_ngrams=True  # N-gram ì‚¬ìš©
    )
    hybrid_ngrams.index_comments(comments)
    results_hybrid_ngrams = hybrid_ngrams.search(query, top_k=3)

    for i, (comment, score, breakdown) in enumerate(results_hybrid_ngrams, 1):
        print(f"\n[{i}] Score: {score:.3f} (BM25: {breakdown['bm25']:.3f}, Semantic: {breakdown['semantic']:.3f})")
        print(f"    ì‘ì„±ì: {comment.get('author', 'Unknown')}")
        print(f"    ë‚´ìš©: {comment['content'][:150]}...")

    # 4. BM25 dominant
    print(f"\n\n{'='*80}")
    print("ã€4ã€‘ Keyword-Focused Hybrid (60% BM25 + 40% Semantic, N-grams)")
    print("-" * 80)
    hybrid_keyword = OptimizedHybridSearch(
        bm25_weight=0.6,
        semantic_weight=0.4,
        use_ngrams=True
    )
    hybrid_keyword.index_comments(comments)
    results_hybrid_keyword = hybrid_keyword.search(query, top_k=3)

    for i, (comment, score, breakdown) in enumerate(results_hybrid_keyword, 1):
        print(f"\n[{i}] Score: {score:.3f} (BM25: {breakdown['bm25']:.3f}, Semantic: {breakdown['semantic']:.3f})")
        print(f"    ì‘ì„±ì: {comment.get('author', 'Unknown')}")
        print(f"    ë‚´ìš©: {comment['content'][:150]}...")


if __name__ == "__main__":
    # ì‹¤ì œ ë°ì´í„° ë¡œë“œ
    issue_file = "data/crawl_sessions/OpenFrame_TPETIME_20260103_045204/347863_20260103_045306.json"

    with open(issue_file, 'r', encoding='utf-8') as f:
        issue = json.load(f)

    comments = issue.get('comments', [])

    # ë‹¤ì–‘í•œ ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸
    test_queries = [
        "TPETIME ì—ëŸ¬ ì›ì¸",           # í•œêµ­ì–´
        "timeout error solution",      # ì˜ì–´
        "batch job failure",           # ê¸°ìˆ  ìš©ì–´
        "ë¬¸ì œ í•´ê²° ë°©ë²•"                # í•œêµ­ì–´ ì¼ë°˜
    ]

    for query in test_queries:
        compare_search_methods(comments, query)
        print("\n\n" + "="*80 + "\n")
