"""
Semantic Search Example for IMS Issue Comments
Uses Sentence Transformers for multilingual semantic similarity
"""
import json
from pathlib import Path
from sentence_transformers import SentenceTransformer, util
import torch

class CommentSemanticSearch:
    """ì‹œë§¨í‹± ê²€ìƒ‰ ì—”ì§„ for IMS ì´ìŠˆ ëŒ“ê¸€"""

    def __init__(self, model_name='paraphrase-multilingual-MiniLM-L12-v2'):
        """
        Args:
            model_name: ë‹¤êµ­ì–´ ì„ë² ë”© ëª¨ë¸
                - paraphrase-multilingual-MiniLM-L12-v2 (ì¶”ì²œ, 50+ ì–¸ì–´)
                - distiluse-base-multilingual-cased-v2 (ë” ë¹ ë¦„)
        """
        print(f"Loading model: {model_name}")
        self.model = SentenceTransformer(model_name)

    def find_best_comments(self, comments, query, top_k=3, threshold=0.3):
        """
        ì¿¼ë¦¬ì™€ ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ëŒ“ê¸€ ì°¾ê¸°

        Args:
            comments: ëŒ“ê¸€ ë¦¬ìŠ¤íŠ¸ [{'content': '...', 'author': '...', ...}]
            query: ê²€ìƒ‰ í‚¤ì›Œë“œ (í•œ/ì˜/ì¼ í˜¼ìš© ê°€ëŠ¥)
            top_k: ìƒìœ„ kê°œ ë°˜í™˜
            threshold: ìµœì†Œ ìœ ì‚¬ë„ (0~1, 0.3 ì´ìƒ ê¶Œì¥)

        Returns:
            List of (comment, score) tuples
        """
        if not comments:
            return []

        # ëŒ“ê¸€ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        contents = [c['content'] for c in comments]

        # ì„ë² ë”© ìƒì„±
        query_embedding = self.model.encode(query, convert_to_tensor=True)
        comment_embeddings = self.model.encode(contents, convert_to_tensor=True)

        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        cosine_scores = util.cos_sim(query_embedding, comment_embeddings)[0]

        # ìƒìœ„ kê°œ ì„ íƒ
        top_results = torch.topk(cosine_scores, k=min(top_k, len(comments)))

        # ê²°ê³¼ í•„í„°ë§ (threshold ì´ìƒë§Œ)
        results = []
        for score, idx in zip(top_results.values, top_results.indices):
            score_value = score.item()
            if score_value >= threshold:
                results.append((comments[idx], score_value))

        return results

    def search_issue_file(self, issue_json_path, query, top_k=3):
        """
        IMS ì´ìŠˆ JSON íŒŒì¼ì—ì„œ ê´€ë ¨ ëŒ“ê¸€ ê²€ìƒ‰

        Args:
            issue_json_path: ì´ìŠˆ JSON íŒŒì¼ ê²½ë¡œ
            query: ê²€ìƒ‰ ì¿¼ë¦¬
            top_k: ìƒìœ„ kê°œ ë°˜í™˜

        Returns:
            List of (comment_dict, similarity_score)
        """
        with open(issue_json_path, 'r', encoding='utf-8') as f:
            issue_data = json.load(f)

        comments = issue_data.get('comments', [])
        if not comments:
            print(f"No comments found in {issue_json_path}")
            return []

        return self.find_best_comments(comments, query, top_k)


# ============================================================
# ì‚¬ìš© ì˜ˆì‹œ
# ============================================================

if __name__ == "__main__":
    # ê²€ìƒ‰ ì—”ì§„ ì´ˆê¸°í™”
    searcher = CommentSemanticSearch()

    # ì˜ˆì‹œ 1: ë‹¨ì¼ ì´ìŠˆ íŒŒì¼ ê²€ìƒ‰
    issue_file = "data/crawl_sessions/OpenFrame_TPETIME_20260103_045204/347863_20260103_045306.json"
    query = "TPETIME ì—ëŸ¬ ì›ì¸"  # í•œêµ­ì–´ ì¿¼ë¦¬

    print(f"\nğŸ” Query: '{query}'")
    print("=" * 80)

    results = searcher.search_issue_file(issue_file, query, top_k=3)

    for i, (comment, score) in enumerate(results, 1):
        print(f"\n[{i}] ìœ ì‚¬ë„: {score:.3f}")
        print(f"ì‘ì„±ì: {comment.get('author', 'Unknown')}")
        print(f"ë‚ ì§œ: {comment.get('created_date', 'N/A')}")
        print(f"ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°:\n{comment['content'][:200]}...")

    # ì˜ˆì‹œ 2: ì—¬ëŸ¬ ì´ìŠˆ ê²€ìƒ‰ (ì„¸ì…˜ í´ë” ì „ì²´)
    print("\n\n" + "=" * 80)
    print("ğŸ“ Searching across all issues in session folder...")
    print("=" * 80)

    session_folder = Path("data/crawl_sessions/OpenFrame_TPETIME_20260103_045204")
    query2 = "timeout error solution"  # ì˜ì–´ ì¿¼ë¦¬

    all_results = []
    for json_file in session_folder.glob("*.json"):
        with open(json_file, 'r', encoding='utf-8') as f:
            issue = json.load(f)

        comments = issue.get('comments', [])
        if not comments:
            continue

        # ê° ì´ìŠˆì—ì„œ ìµœê³  ì ìˆ˜ ëŒ“ê¸€ë§Œ
        best_results = searcher.find_best_comments(comments, query2, top_k=1, threshold=0.4)

        if best_results:
            comment, score = best_results[0]
            all_results.append({
                'issue_id': issue.get('issue_id'),
                'title': issue.get('title'),
                'comment': comment,
                'score': score
            })

    # ì ìˆ˜ìˆœ ì •ë ¬
    all_results.sort(key=lambda x: x['score'], reverse=True)

    print(f"\nğŸ” Query: '{query2}'")
    print(f"Found {len(all_results)} relevant comments across {len(list(session_folder.glob('*.json')))} issues\n")

    for i, result in enumerate(all_results[:5], 1):  # Top 5
        print(f"\n[{i}] Issue {result['issue_id']}: {result['title'][:60]}...")
        print(f"    ìœ ì‚¬ë„: {result['score']:.3f}")
        print(f"    ëŒ“ê¸€: {result['comment']['content'][:150]}...")
